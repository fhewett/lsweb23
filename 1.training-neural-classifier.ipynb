{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01d06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training notebook for easy vs. standard German classifier \n",
    "# Authors: Hadi Asghari & Freya Hewett\n",
    "# Version: 2023.02\n",
    "\n",
    "# Note: due to copyright reasons we can only release a subset of our dataset, therefore if you re-run this \n",
    "# notebook, the results may differ\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce6d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Limit GPU memory use by TF\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c31b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10164 \n",
      "   source                                               text  label\n",
      "0  klexi  Das Musical [ˈmju:zikəl] ist eine in der Regel...      0\n",
      "validation: 2530 \n",
      "   source                                               text  label\n",
      "0  klexi  Der Leopard (Panthera pardus) ist eine Art aus...      0\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASETS  \n",
    "train_pds = pd.read_csv(\"./data/dataset-train.csv\", sep=\"\\t\")\n",
    "print(\"train:\", len(train_pds), \"\\n\", train_pds.head(1))\n",
    "eval_pds = pd.read_csv(\"./data/datset-test.csv\", sep=\"\\t\")\n",
    "print(\"validation:\" , len(eval_pds), \"\\n\", eval_pds.head(1))\n",
    "\n",
    "# TURN DATA INTO TF DATASETS\n",
    "minibatchsize = 32\n",
    "train_x = train_pds.text  # used by keras tokenizers later\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_pds.text, train_pds.label))\n",
    "train_ds = train_ds.batch(minibatchsize)  # needed to allow iteration\n",
    "eval_ds = tf.data.Dataset.from_tensor_slices((eval_pds.text, eval_pds.label))  \n",
    "eval_ds = eval_ds.batch(minibatchsize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90827df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of some basic metrics to make sure our trainingset is in order.\n",
    "print(\"*** balance:\")  \n",
    "print(train_pds.groupby(['source', 'label']).count())\n",
    "\n",
    "print(\"\\n*** check training sample in validation: \")\n",
    "for i, s in enumerate(eval_pds.text):\n",
    "    if len(train_pds[train_pds.text==s]):\n",
    "        print('repetitive val. sample:', i)  \n",
    "        print(s, \"\\n\")\n",
    "\n",
    "print(\"\\n*** basic complexity:\")\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "tkz = RegexpTokenizer(r'\\w+')\n",
    "train_pds[\"words\"] = train_pds['text'].apply(lambda x: len(tkz.tokenize(x)))\n",
    "train_pds[\"wordlen\"] = train_pds['text'].str.len() / train_pds[\"words\"]\n",
    "train_pds[\"sentlen\"] = train_pds[\"words\"] / train_pds['text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "train_pds.groupby(['source', 'label']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb32bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 12:04:50.678505: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-06-14 12:04:50.730990: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing / tokenizer for BoW model\n",
    "max_tokens = 20000\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_vectorization.adapt(train_x)\n",
    "\n",
    "train_v = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "eval_v = eval_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbde926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "['[UNK]', 'die', 'der', 'in', 'und', 'das', 'den', 'ist', 'im', 'von']\n",
      "['unangenehm', 'umzugehen', 'umweltminister', 'umweltaktivistin', 'umweg', 'umrunden', 'umkreis', 'umgeknickte', 'umgegangen', 'umgebenden']\n"
     ]
    }
   ],
   "source": [
    "vocab = text_vectorization.get_vocabulary()\n",
    "print(len(vocab))  # we have 20k words\n",
    "print(vocab[:10])  # words in order of commonality \n",
    "print(vocab[-10:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e9ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "  9/321 [..............................] - ETA: 2s - loss: 0.3929 - accuracy: 0.7604  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 13:26:45.584220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/321 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 13:26:47.989713: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 3s 8ms/step - loss: 0.1370 - accuracy: 0.9428 - val_loss: 0.0578 - val_accuracy: 0.9825\n",
      "Epoch 2/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0273 - accuracy: 0.9912 - val_loss: 0.0285 - val_accuracy: 0.9910\n",
      "Epoch 3/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.0249 - val_accuracy: 0.9926\n",
      "Epoch 4/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0263 - val_accuracy: 0.9930\n",
      "Epoch 5/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0306 - val_accuracy: 0.9930\n",
      "Epoch 6/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0327 - val_accuracy: 0.9930\n",
      "Epoch 7/10\n",
      "321/321 [==============================] - 3s 8ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0380 - val_accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0447 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "321/321 [==============================] - 3s 8ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0477 - val_accuracy: 0.9910\n",
      "Epoch 10/10\n",
      "321/321 [==============================] - 2s 8ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0538 - val_accuracy: 0.9899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2821861f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-layers fully connected architecture for BoW\n",
    "\n",
    "def get_model(max_tokens=max_tokens, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "\n",
    "model.fit(\n",
    "    train_v,\n",
    "    validation_data=eval_v,\n",
    "    epochs=10,\n",
    "    callbacks=[ModelCheckpoint('modelcheckpoint', save_best_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07cdc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/81 [============>.................] - ETA: 0s - loss: 0.0416 - accuracy: 0.9873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 13:01:33.160126: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9914\n",
      "[0.027651270851492882, 0.9914330840110779]\n"
     ]
    }
   ],
   "source": [
    "# Test model. Note accuracy is .99\n",
    "model = keras.models.load_model('modelcheckpoint')\n",
    "print(model.evaluate(eval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990e731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 11:10:32.054712: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model-nnbow-bigger/assets\n"
     ]
    }
   ],
   "source": [
    "# SAVE MODEL\n",
    "model.save(\"mbow-alldata\")\n",
    "\n",
    "pickle.dump({'config': text_vectorization.get_config(), 'weights': text_vectorization.get_weights()},\n",
    "            open(\"textvectorizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbbe91",
   "metadata": {},
   "source": [
    "**Additional notes**\n",
    "\n",
    "- We experimented with more complex models as well, but the accuracy of the model in this notebook was sufficient (.99 on test set)\n",
    "- The model was tested on LS pages from states: 286 (78%) out of 365 pages received a prediction of >0.7 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
