{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create subset of Oscar22 domains that are within Curlie and end with  .de\n",
    "\n",
    "- Author: Hadi Asghari\n",
    "- Version: 2023.02\n",
    "\n",
    "- __input__: curlie-ourset.csv  &  oscar 22.01 data \n",
    "- __output__: oscar22-subset.jsonl (~100GB uncompressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pickle\n",
    "import binascii\n",
    "import zlib\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading OSCAR 22 data**\n",
    "\n",
    "- The Oscar dataset is a curated version of the common crawl.\n",
    "- You can download this data from: https://oscar-project.github.io/documentation/versions/oscar-2201/\n",
    "- Fields of interest include `content` & `warc-headers/warc-target-uri`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curlie = pd.read_csv(\".data/curlie-ourset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO OVER THE JSONL OSCAR DATA AND KEEP ROWS WITH URLS IN OUR FILTER SET\n",
    "\n",
    "# NOTE, Following code requires around 64GB RAM to run\n",
    "# (this is with the aid of zlib string compression)\n",
    "\n",
    "DATADIR = \"./data/oscar22/\"\n",
    "PARTS = 497\n",
    "filter_set = set(curlie.domain)\n",
    "subset, uris, ignored = dict(), Counter(), set()\n",
    "dates = Counter()\n",
    "\n",
    "st = time()\n",
    "for part in range (1, PARTS+1):\n",
    "    # each _part_ takes about 11s, and has about 150k records \n",
    "    print(part, end=' .. ')\n",
    "    with open(DATADIR + f\"de_meta_part_{part}.jsonl\") as f:\n",
    "        for js in f.readlines():\n",
    "            m = json.loads(js)\n",
    "            content = m['content']\n",
    "            uri = m['warc_headers']['warc-target-uri']\n",
    "            dt = m['warc_headers']['warc-date'][:10]\n",
    "\n",
    "            # check if URI within our target\n",
    "            fdomain = tldextract.extract(uri).fqdn.lower()\n",
    "            if fdomain.startswith(\"www.\"):\n",
    "                fdomain = fdomain[4:]\n",
    "            if domain.startswith(\"m.\"):\n",
    "                fdomain = fdomain[2:]\n",
    "            if domain.startswith(\"de.\"):\n",
    "                fdomain = fdomain[3:]\n",
    "            rdomain = tldextract.extract(domain).registered_domain\n",
    "\n",
    "            if fdomain not in filter_set and rdomain not in filter_set:\n",
    "                ignored.add(uri)\n",
    "                continue  # missing!\n",
    "\n",
    "            # stats re zlib compression (reduces memory to a third and relatively fast)\n",
    "            content = zlib.compress(bytes(content, 'utf-8'))  \n",
    "\n",
    "            dates[dt] += 1\n",
    "            uris[uri] += 1\n",
    "            if not uri in subset:\n",
    "                subset[uri] = content\n",
    "            else:\n",
    "                # if exists: keep all content versions in a list (unless duplicate content)\n",
    "                if not type(subset[uri]) is list:\n",
    "                    subset[uri] = [subset[uri]]\n",
    "                dupdup = False\n",
    "                for c in subset[uri]:\n",
    "                    if c == content:\n",
    "                        dupdup = True\n",
    "                if not dupdup:\n",
    "                    subset[uri].append(content)\n",
    "                else:\n",
    "                    # exact duplicate content, ignore this item, and possibly flatten list\n",
    "                    if len(subset[uri]) == 1:\n",
    "                        subset[uri] = subset[uri][0]\n",
    "                        uris[uri] -= 1\n",
    "        # break\n",
    "#\n",
    "print(\"=> done in \", round(time()-st), \"seconds.\\n\")  # ~6000s \n",
    "print(\"Dates: \", min(dates), max(dates))  # crawl dates (two weeks between 26.11.2021 to 09.12.2021)\n",
    "\n",
    "# interim save for reuse\n",
    "# with open(\"subset-curlide.p\", \"wb\") as f:\n",
    "#    pickle.dump(subset, f)\n",
    "\n",
    "# stats\n",
    "print(\"URIs in/out:\", len(uris), len(ignored))\n",
    "dups = [k for k,v in uris.items() if v>1]\n",
    "print(\"Duplicate URIs:\", len(dups))  # 32804  (<1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE FILTERED SUBSET TO A NEW JSONL FILE\n",
    "# - regarding duplicate URIs: we keep the first (<1%)\n",
    "# - the jsonl's will be uncompressed (size: ~100GB)\n",
    "# - we'll keep the default dictionary order (which is as read/added in py3.10+) vs sorting by domain or category;\n",
    "\n",
    "if not 'curix' in locals():\n",
    "    curix = curlie.set_index('domain')\n",
    "\n",
    "st = time()\n",
    "\n",
    "OUTFILE = \"oscar22-subset.jsonl\"\n",
    "assert not path.exists(OUTFILE)\n",
    "with open(OUTFILE, \"wt\") as f:\n",
    "    for i, (url, content) in enumerate(subset.items()):\n",
    "        if i%100000 == 0:  # takes around ~12s\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        if type(content) is list:\n",
    "            content = content[0]\n",
    "        content = zlib.decompress(content).decode('utf-8')\n",
    "        domain = tldextract.extract(url).registered_domain.lower()\n",
    "        domcat = curix.loc[domain, 'cat']\n",
    "        js = json.dumps({'url': url, 'domain': domain, 'domcat': domcat, 'content': content})\n",
    "        f.write(js + \"\\n\")\n",
    "        #break\n",
    "\n",
    "print(\"\\n=> wrote jsonl in\", round(time()-st), \"secs.\")  # 1922s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
