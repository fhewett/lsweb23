{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create curated Curlie dataset (flattened categories for .de domains)\n",
    "\n",
    "This means the curlie categories for each domain are condensed into one based on some elaborate heuristics\n",
    "\n",
    "- Author: Hadi Asghari\n",
    "- Version: 2023.02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input__: curated curlie dataset from H2V project (links below) \n",
    "- __Output__: curlie-ourset.csv (this is already provided so rerunning this notebook is unnecessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pickle\n",
    "import binascii\n",
    "import zlib\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-24 16:52:01--  https://figshare.com/ndownloader/files/34491131\n",
      "Resolving figshare.com (figshare.com)... 99.81.233.31, 46.137.13.70, 2a05:d018:1f4:d000:614f:f7d5:b342:899c, ...\n",
      "Connecting to figshare.com (figshare.com)|99.81.233.31|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/34491131/curlie.csv.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230224/eu-west-1/s3/aws4_request&X-Amz-Date=20230224T155201Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=1d0a57adb0835cb060322e43d2b5d79f03bafceaab0e5e6572949a0944144241 [following]\n",
      "--2023-02-24 16:52:01--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/34491131/curlie.csv.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230224/eu-west-1/s3/aws4_request&X-Amz-Date=20230224T155201Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=1d0a57adb0835cb060322e43d2b5d79f03bafceaab0e5e6572949a0944144241\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.17.8, 52.218.80.92, 52.218.24.203, ...\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.17.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37245213 (36M) [application/gzip]\n",
      "Saving to: ‘34491131’\n",
      "\n",
      "34491131            100%[===================>]  35,52M  5,49MB/s    in 10s     \n",
      "\n",
      "2023-02-24 16:52:12 (3,49 MB/s) - ‘34491131’ saved [37245213/37245213]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download as input the Curlie curated dataset by the H2V project \n",
    "# This is necessary as Curlie has removed archives of their own data\n",
    "# However, we use our own heuristics instead of H2V's categories as theirs is a bit too broad \n",
    "# and exlucdes regional which is very important in Germany (at least in v3)\n",
    "\n",
    "# H2V project: https://github.com/epfl-dlab/homepage2vec \n",
    "# Data repository: https://figshare.com/ndownloader/files/38937971 \n",
    "# The file is curlie.csv.gz (unfiltered) version 3.\n",
    "!wget https://figshare.com/ndownloader/files/34491131\n",
    "!mv 34491131 ./data/curlie-by-h2v.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting f/r domains...\n",
      "\n",
      "curlie data records/domains: 1467914 1210441\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "      <th>fdomain</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>www.malaysiakini.com</td>\n",
       "      <td>/en/Regional/Asia/Malaysia/News_and_Media</td>\n",
       "      <td>en</td>\n",
       "      <td>malaysiakini.com</td>\n",
       "      <td>malaysiakini.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>www.bernama.com</td>\n",
       "      <td>/en/Regional/Asia/Malaysia/News_and_Media</td>\n",
       "      <td>en</td>\n",
       "      <td>bernama.com</td>\n",
       "      <td>bernama.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     url                                      label lang  \\\n",
       "14  www.malaysiakini.com  /en/Regional/Asia/Malaysia/News_and_Media   en   \n",
       "16       www.bernama.com  /en/Regional/Asia/Malaysia/News_and_Media   en   \n",
       "\n",
       "             fdomain            domain  \n",
       "14  malaysiakini.com  malaysiakini.com  \n",
       "16       bernama.com       bernama.com  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: load the H2V raw Curlie data (downloaded above)\n",
    "\n",
    "# we extract two domain parts: the FQDN (includes subdomain, minus www./m./de.) and the registered domain part; in this notebook we shall base the analysis on the r-domain\n",
    "# we keep only `de` & `en` language part of the Curlie data, and futher filter domains to `.de`\n",
    "\n",
    "curlie_ = pd.read_csv(\"./data/curlie-by-h2v.csv.gz\")  # 3s to load; note `uid` repeats hence is not index\n",
    "curlie_.drop(columns=['Unnamed: 0', 'uid'], inplace=True)\n",
    "\n",
    "# en + de are the biggest communities on Curlie -- we limit to these two groups\n",
    "curlie1 = curlie_[curlie_.lang.isin(['de', 'en'])].copy()\n",
    "\n",
    "# remove some wierd UTF code in the file\n",
    "def convert_htmlperc(s):\n",
    "    ll = s.split('%')\n",
    "    bs = bytes(ll[0], encoding=\"UTF-8\")\n",
    "    for l in ll[1:]:\n",
    "        bs += binascii.unhexlify(l[0:2]) + bytes(l[2:], encoding=\"UTF-8\")\n",
    "    return bs.decode()\n",
    "\n",
    "curlie1.label = curlie1.label.apply(convert_htmlperc)\n",
    "\n",
    "# extract full/reg domains.\n",
    "# (note, removal of www/de maybe problematic on some very short domain names, but we can ignore for now)\n",
    "print('extracting f/r domains...')\n",
    "curlie1.loc[curlie1.url==\"www.netzgeek.de%20title=\", \"url\"] = \"www.netzgeek.de\"  # manually fix 1 error\n",
    "curlie1['fdomain'] = curlie1.url.apply(lambda x: tldextract.extract(x).fqdn)  # 10s+\n",
    "curlie1['fdomain'] = curlie1.fdomain.str.replace(r\"^www[.]\", \"\", regex=True)\n",
    "curlie1['fdomain'] = curlie1.fdomain.str.replace(r\"^m[.]\", \"\", regex=True)\n",
    "curlie1['fdomain'] = curlie1.fdomain.str.replace(r\"^de[.]\", \"\", regex=True)\n",
    "curlie1['domain'] = curlie1.url.apply(lambda x: tldextract.extract(x).registered_domain)  # 10s+\n",
    "curlie1 = curlie1[curlie1.domain!=\"\"]  # drop the few empty ones\n",
    "\n",
    "# num of labels based on `domain` (rdomain is much larger)\n",
    "print(\"\\ncurlie data records/domains:\", len(curlie1), len(set(curlie1.domain)))\n",
    "curlie1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245060\n"
     ]
    }
   ],
   "source": [
    "# SPECIFIC TO OUR RESEARCH DESIGN: LIMIT CURLIE ONLY TO .DE/.AT DOMAINS \n",
    "# This is done to speed up processing in this file. (we eventually only use the .de in the paper)\n",
    "curlie1 = curlie1[curlie1.domain.str[-3:].isin(['.de', '.at'])].copy()  # 1210k => .de: 228k + .at: 17k\n",
    "print(len(set(curlie1.domain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286231\n",
      "cat\n",
      "Arts              18072\n",
      "Business          75483\n",
      "Computers          7262\n",
      "Education          9228\n",
      "Games              2225\n",
      "Government         1607\n",
      "Guide               207\n",
      "Health            22190\n",
      "Home               1197\n",
      "Kids_and_Teens     1295\n",
      "News               2240\n",
      "Recreation        36526\n",
      "Reference           459\n",
      "RegOther          49637\n",
      "Science            5614\n",
      "Shopping           8060\n",
      "Society           27786\n",
      "Travel            17143\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: let's add the single-word-category additions stuff\n",
    "# from the curlie category/label, we keep the first/top category for all categories except 'regional'. \n",
    "# for regional, we look at the next relevant labels \n",
    "# for an explanation of the actual categories see curlie.org\n",
    "\n",
    "# get the top category (ignoring the starting /de/ or /en/)\n",
    "curlie1['cat'] = curlie1.label.apply(lambda x: x.split('/',3)[2])\n",
    "curlie1['catreg'] = False\n",
    "\n",
    "# combine the German & English labels (we do this manually)\n",
    "curlie1.loc[curlie1.cat=='Computer', 'cat'] = 'Computers'\n",
    "curlie1.loc[curlie1.cat=='Freizeit', 'cat'] = 'Recreation'\n",
    "curlie1.loc[curlie1.cat=='Gesellschaft', 'cat'] = 'Society'\n",
    "curlie1.loc[curlie1.cat=='Gesundheit', 'cat'] = 'Health'\n",
    "curlie1.loc[curlie1.cat=='Kultur', 'cat'] = 'Arts'\n",
    "curlie1.loc[curlie1.cat=='Medien', 'cat'] = 'News'\n",
    "curlie1.loc[curlie1.cat=='Online-Shops', 'cat'] = 'Shopping'\n",
    "curlie1.loc[curlie1.cat=='Spiele', 'cat'] = 'Games'\n",
    "curlie1.loc[curlie1.cat=='Sport', 'cat'] = 'Recreation'  # this is deu\n",
    "curlie1.loc[curlie1.cat=='Sports', 'cat'] = 'Recreation'  # this is eng\n",
    "curlie1.loc[curlie1.cat=='Wirtschaft', 'cat'] = 'Business'\n",
    "curlie1.loc[curlie1.cat=='Wissen', 'cat'] = 'Reference'\n",
    "curlie1.loc[curlie1.cat=='Wissenschaft', 'cat'] = 'Science'\n",
    "curlie1.loc[curlie1.cat=='Zuhause', 'cat'] = 'Home'\n",
    "\n",
    "# Reference/Education we split as Education; this is important re all .edu/univ websites, for instance\n",
    "for ix, row in curlie1[curlie1.cat=='Reference'].iterrows():\n",
    "    if row['label'].startswith(\"/en/Reference/Education/\") or row['label'].startswith(\"/de/Wissen/Bildung/\"):\n",
    "        curlie1.loc[ix, 'cat'] = \"Education\"\n",
    "\n",
    "# Similarly, we make Recreation/Travel it's own category (given the many regional sites)\n",
    "for ix, row in curlie1[curlie1.cat=='Recreation'].iterrows():\n",
    "    if row['label'].startswith(\"/en/Recreation/Travel/\") or row['label'].startswith(\"/de/Freizeit/Reisen/\"):\n",
    "        curlie1.loc[ix, 'cat'] = \"Travel\"\n",
    "\n",
    "# Let's further unpack `regional` (only German)\n",
    "# we use first match from left\n",
    "# these categories can later on be merged with the top categories\n",
    "for ix, row in curlie1[curlie1.cat=='Regional'].iterrows():\n",
    "    lbl = row['label']\n",
    "    cat = None\n",
    "    for ll in  lbl.split('/'):\n",
    "        if ll == \"Society_and_Culture\" or ll == 'Gesellschaft':\n",
    "            cat = 'Society'\n",
    "            break\n",
    "        elif ll == \"Business_and_Economy\"  or ll == \"Wirtschaft\":\n",
    "            cat = \"Business\"\n",
    "            break\n",
    "        elif ll == \"Arts_and_Entertainment\" or ll == \"Kultur\":\n",
    "            cat = \"Arts\"\n",
    "            break\n",
    "        elif ll == \"Health\" or ll == \"Gesundheit\":\n",
    "            cat = \"Health\"\n",
    "            break\n",
    "        elif ll == \"Education\" or ll == \"Bildung\":\n",
    "            cat = \"Education\"\n",
    "            break\n",
    "        elif ll == \"Science_and_Environment\" or ll == \"Natur_und_Umwelt\":\n",
    "            cat = \"Science\"\n",
    "            break\n",
    "        elif ll in (\"Travel_and_Tourism\", \"Transportation\", \"Transport\", \"Reise_und_Tourismus\", \"Verkehr\", \"Gastgewerbe\"):\n",
    "            cat = \"Travel\"\n",
    "            break\n",
    "        elif ll == \"Recreation_and_Sports\" or ll == \"Sport\" or ll == \"Freizeit\":\n",
    "            cat = \"Recreation\"  # merged with sports\n",
    "            break\n",
    "        elif ll ==\"Government\" or ll == \"Staat\" or ll ==\"Ämter\":\n",
    "            cat = \"Government\"  # incs. member of parliament\n",
    "            break\n",
    "        elif ll == \"News_and_Media\" or ll == \"Nachrichten_und_Medien\" or ll == \"Weather\" or ll == \"Wetter\":\n",
    "            cat = \"News\"  # mix of news & media\n",
    "            break\n",
    "        elif ll == \"Guides_and_Directories\" or ll == \"Verzeichnisse_und_Portale\":\n",
    "            cat = \"Guide\"  # in regions, these guides are often run by local governments\n",
    "            break\n",
    "    if not cat:\n",
    "        # what remains (incls Provinces; States_and_Federal_Territories; Districts; Regions; Maps_and_Views...)\n",
    "        cat = 'RegOther'\n",
    "    curlie1.loc[ix, 'cat'] = cat\n",
    "    curlie1.loc[ix, 'catreg'] = True\n",
    "\n",
    "# summarize so far\n",
    "print(len(curlie1))\n",
    "print(curlie1.groupby('cat').label.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.63 s, sys: 40 ms, total: 6.66 s\n",
      "Wall time: 6.66 s\n",
      "fqdomains: 252162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdomain</th>\n",
       "      <th>cat</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007-berlin.de</td>\n",
       "      <td>Society</td>\n",
       "      <td>007-berlin.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>011.joomla.schule.bremen.de</td>\n",
       "      <td>Education</td>\n",
       "      <td>bremen.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01art.de</td>\n",
       "      <td>Business</td>\n",
       "      <td>01art.de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       fdomain        cat         domain\n",
       "0                007-berlin.de    Society  007-berlin.de\n",
       "1  011.joomla.schule.bremen.de  Education      bremen.de\n",
       "2                     01art.de   Business       01art.de"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 3: MERGE categories by fdomain\n",
    "# We condense our frame into a frame grouped by domain, \n",
    "# and combine the labels into one, based on some heurestics derived from manual inspection\n",
    "\n",
    "%time curlie2 = curlie1.groupby(['fdomain'], as_index=False).agg({'cat': set, 'domain': set})\n",
    "curlie2['cat'] = curlie2.cat.apply(lambda x: str(sorted(x))[1:-1].replace(\"'\", \"\").replace(\",\", \" \"))\n",
    "curlie2['domain'] = curlie2.domain.apply(lambda x: str(sorted(x))[1:-1].replace(\"'\", \"\"))\n",
    "\n",
    "# Now, iterate & merge categories (11 cats max per domain)\n",
    "# - note in both losing & dominating categories, the order is important (as to what cat is left standing)\n",
    "# - in general, the larger category/groups can be replaced with more specific ones\n",
    "\n",
    "# 1. LOSING CATEGORIES :   CAT & X => X\n",
    "for i, row in curlie2.iterrows():\n",
    "    cat = row['cat']\n",
    "    if \"RegOther\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"RegOther\", \"\").strip()\n",
    "    if \"Business\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Business\", \"\").strip()\n",
    "    if \"Reference\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Reference\", \"\").strip()\n",
    "    if \"Society\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Society\", \"\").strip()\n",
    "    if \"Recreation\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Recreation\", \"\").strip()\n",
    "    if \"Computers\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Computers\", \"\").strip()\n",
    "    if \"News\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"News\", \"\").strip()\n",
    "    if \"Science\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Science\", \"\").strip()\n",
    "    if \"Home\" in cat and \" \" in cat:\n",
    "        cat = cat.replace(\"Home\", \"\").strip()\n",
    "    curlie2.loc[i, \"cat\"] = cat\n",
    "\n",
    "# 2. DOMINATING CATEGORIES :  CAT & X => CAT!\n",
    "c = Counter(curlie2.cat)\n",
    "for k,v in c.items():\n",
    "    if \"Government\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Government\"\n",
    "    elif \"Kids_and_Teens\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Kids_and_Teens\"\n",
    "    elif \"Travel\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Travel\"\n",
    "    elif \"Arts\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Arts\"\n",
    "    elif \"Health\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Health\"\n",
    "    elif \"Shopping\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Shopping\"\n",
    "    elif \"Education\" in k and \" \" in k:\n",
    "        curlie2.loc[curlie2.cat==k, \"cat\"] = \"Education\"\n",
    "\n",
    "assert len(curlie2[curlie2.cat.str.contains(\" \")]) == 0  # sanity check\n",
    "\n",
    "# save interim results\n",
    "# with open(\"./data/tmp-curlie-fdomain.p\", \"wb\") as f:  \n",
    "#    pickle.dump(curlie2, f)\n",
    "\n",
    "print('fqdomains:', len(curlie2))\n",
    "curlie2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.22 s, sys: 11.9 ms, total: 6.23 s\n",
      "Wall time: 6.23 s\n",
      "in 181 s\n",
      "domains: 245060\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>cats</th>\n",
       "      <th>fdoms</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007-berlin.de</td>\n",
       "      <td>{'Society': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007box.de</td>\n",
       "      <td>{'Computers': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01art.de</td>\n",
       "      <td>{'Business': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain              cats  fdoms        cat\n",
       "0  007-berlin.de    {'Society': 1}      1    Society\n",
       "1      007box.de  {'Computers': 1}      1  Computers\n",
       "2       01art.de   {'Business': 1}      1   Business"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 4: FURTHER MERGE INTO registered DOMAIN (combining subdomains)\n",
    "# - we condense using heuristics, MAJORITY VOTING, and tie-breaker rules similar to for fdomain\n",
    "# - there are a few large domains left as 'MIX' which are manually reclassified later\n",
    "\n",
    "%time curlie3 = curlie2.groupby(['domain'], as_index=False).agg({'cat': Counter, 'fdomain': len})\n",
    "curlie3.rename(columns={'cat': 'cats', 'fdomain':'fdoms'}, inplace=True)\n",
    "# note, I also tried grouping by curlie.domain, as then the weights will slightly differ\n",
    "#       however, upon manual inspection, that wasn't really better\n",
    "\n",
    "# main loop to combine categories per domain\n",
    "st = time()\n",
    "curlie3['cat'] = ''\n",
    "for i, row in curlie3.iterrows():\n",
    "    cats, fdoms = row['cats'].copy(), row['fdoms']\n",
    "\n",
    "    # first some simple heuristics\n",
    "    # - if RegOther and other categories, let's keep the other ones\n",
    "    # - if both science & education, let's merge them (most are higher-ed)\n",
    "    if len(cats) > 1 and 'RegOther' in cats:\n",
    "        cats.pop('RegOther')\n",
    "    if 'Education' in cats and 'Science' in cats:\n",
    "        cats['Education'] += cats['Science']\n",
    "        cats.pop('Science')\n",
    "\n",
    "    # now, if we have just one category, use that\n",
    "    if len(cats) == 1:\n",
    "        curlie3.loc[i, 'cat'] = list(cats.keys())[0]\n",
    "        continue\n",
    "\n",
    "    # next, if there are many subdomains, use majority voting (or MIX);\n",
    "    #            if few subdomains, use top category, unless tied\n",
    "    (cat1, n1), (cat2, n2) = cats.most_common(2)\n",
    "    if fdoms > 99:\n",
    "        if n1 / fdoms > 0.5:\n",
    "            curlie3.loc[i, 'cat'] = cat1\n",
    "        else:\n",
    "            curlie3.loc[i, 'cat'] = 'MIX'\n",
    "    else:\n",
    "        if n1 > n2:\n",
    "            curlie3.loc[i, 'cat'] = cat1\n",
    "        else:\n",
    "            # finally, for ties: use the heuristics/rules we used for fdomains (curlie2)\n",
    "            # we check for any dominating categories, otherwise, remove categories until we get to 1...\n",
    "            top = [c for c,n in cats.items() if n==n1]\n",
    "            if 'Government' in top:\n",
    "                curlie3.loc[i, 'cat'] = 'Government'\n",
    "            elif 'Kids_and_Teens' in top:\n",
    "                curlie3.loc[i, 'cat'] = 'Kids_and_Teens'\n",
    "            elif 'Travel' in top:\n",
    "                curlie3.loc[i, 'cat'] = 'Travel'\n",
    "            elif 'Arts' in top:\n",
    "                curlie3.loc[i, 'cat'] = 'Arts'\n",
    "            elif 'Health' in top:\n",
    "                curlie3.loc[i, 'cat'] = 'Health'\n",
    "            elif 'Shopping' in top:\n",
    "                curlie3.loc[i, 'cat'] = 'Shopping'\n",
    "            elif 'Education' in top:\n",
    "                 curlie3.loc[i, 'cat'] = 'Education'\n",
    "            else:\n",
    "                if len(top) > 1 and 'Business' in top:\n",
    "                    top.remove('Business')\n",
    "                if len(top) > 1 and 'Reference' in top:\n",
    "                    top.remove('Reference')\n",
    "                if len(top) > 1 and 'Society' in top:\n",
    "                    top.remove('Society')\n",
    "                if len(top) > 1 and 'Recreation' in top:\n",
    "                    top.remove('Recreation')\n",
    "                if len(top) > 1 and 'Computers' in top:\n",
    "                    top.remove('Computers')\n",
    "                if len(top) > 1 and 'News' in top:\n",
    "                    top.remove('News')\n",
    "                if len(top) > 1 and 'Science' in top:\n",
    "                    top.remove('Science')\n",
    "                if len(top) > 1 and 'Home' in top:\n",
    "                    top.remove('Home')\n",
    "                assert len(top) == 1  # only 1 at this point \n",
    "                curlie3.loc[i, 'cat'] = top[0]\n",
    "\n",
    "#\n",
    "print(\"in\", round(time()-st), \"s\")  # ~100s\n",
    "\n",
    "# save interim results\n",
    "# with open(\"./data/tmp-curlie-rdomain.p\", \"wb\") as f:  \n",
    "#    pickle.dump(curlie3, f)\n",
    "\n",
    "print('domains:', len(curlie3))\n",
    "print(len(curlie3[curlie3.cat.str.startswith('MIX')]))  \n",
    "curlie3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some individual checks below\n",
    "# - note, in this list, only two appear wrong (xs4all.nl & essen.de)\n",
    "# - the rest appear correct, which is good \n",
    "# curlie[curlie.domain.isin([\"nrw.de\", \"uni-potsdam.de\", \"bayern.de\", \"bayer.de\", \"lmu.de\", \"lego.com\", \"pbs.org\",\n",
    "#                            \"sagepub.com\", \"gmu.edu\", \"xs4all.nl\", \"ucanr.edu\", \"sourceforge.net\",\n",
    "#                            \"uwa.edu.au\", \"drk.de\", \"essen.de\", \"nationalgeographic.com\",\n",
    "#                            \"dlrg.de\", \"eq.edu.au\", \"weebly.com\", \"webs.com\", \"freeservers.com\",\n",
    "#                            \"free.fr\", \"homestead.com\", \"uk.com\", \"house.gov\", \"iheart.com\",\n",
    "#                            \"play-cricket.com\", \"schoolloop.com\", \"ox.ac.uk\",\n",
    "#                            \"freeservers.com\",  \"iwarp.com\", \"jimdofree.com\", \"beepworld.de\",\n",
    "#                            \"sourceforge.net\", \"blogspot.com\", \"wordpress.com\", \"tripod.com\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curlie .de set: 245060\n",
      "\n",
      " cat\n",
      "Arts          16114\n",
      "Business      65234\n",
      "Computers      6860\n",
      "Education      7425\n",
      "Games          2099\n",
      "Government     1171\n",
      "Health        20216\n",
      "Home           1142\n",
      "Kids           1253\n",
      "News           1812\n",
      "Recreation    30685\n",
      "RegOther      40017\n",
      "Science        3751\n",
      "Shopping       7794\n",
      "Society       22911\n",
      "Travel        16576\n",
      "Name: domain, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Final touches before saving\n",
    "curlie = curlie3.copy()\n",
    "\n",
    "# change remaining four 'mix' domains \n",
    "curlie.loc[curlie.domain=='nrw.de', 'cat'] = 'Government'\n",
    "curlie.loc[curlie.domain=='t-online.de', 'cat'] = 'Recreation'  # includes a lot of private pages\n",
    "curlie.loc[curlie.domain=='beepworld.de', 'cat'] = 'Recreation' # similar\n",
    "curlie.loc[curlie.domain=='gmxhome.de', 'cat'] = 'Recreation'   # similar\n",
    "\n",
    "# simplify\n",
    "curlie.loc[curlie.cat=='Kids_and_Teens', 'cat'] = 'Kids'  # simpler name\n",
    "curlie.loc[curlie.cat=='Guide', 'cat']  = 'Travel'  # merge as includes a lot of travel sites\n",
    "curlie.loc[curlie.cat=='Reference', 'cat'] = 'RegOther'  # merge \n",
    "\n",
    "# remove unused column and finally save\n",
    "curlie.drop(columns={'cats', 'fdoms'}, inplace=True)\n",
    "curlie.to_csv(\"./data/curlie-ourset.csv\", index=False)\n",
    "\n",
    "# some stats\n",
    "print('curlie .de/.at set:', len(curlie))\n",
    "print(\"\\n\", curlie.groupby('cat').domain.count(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
